{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    " print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST_PATH ='original\\digit_xtest.csv'\n",
    "X_TRAIN_PATH = 'original\\digit_xtrain.csv'\n",
    "Y_TRAIN_PATH = 'original\\digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'original\\digit_ytest.csv'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_log/'"
   ]
  },
  {
   "source": [
    "# Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "y_test_all = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "x_test_all = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "source": [
    "# Explore"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "source": [
    "# Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Re-scale\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, x_test_all = x_train_all/255.0, x_test_all/255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(10)[y_train_all]\n",
    "y_test_all = np.eye(10)[y_test_all]\n"
   ]
  },
  {
   "source": [
    "# Creating validation dataset from training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_test_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "source": [
    "# Setup Tensorflow Graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES])"
   ]
  },
  {
   "source": [
    "### Neural Network Architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64\n"
   ]
  },
  {
   "source": [
    "#### Layer 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "# with tf.name_scope('hidden_layer1'):\n",
    "\n",
    "#     initial_w1 = tf.truncated_normal(shape=[TOTAL_INPUTS, n_hidden1], stddev=0.01, seed=42)\n",
    "#     w1 = tf.Variable(initial_value=initial_w1)\n",
    "\n",
    "#     initial_b1 = tf.constant(value = 0.0, shape=[n_hidden1])\n",
    "#     b1 = tf.Variable(initial_value = initial_b1)\n",
    "\n",
    "#     layer1_in = tf.matmul(X, w1) + b1\n",
    "#     layer1_out = tf.nn.relu(layer1_in)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 89,
   "outputs": []
  },
  {
   "source": [
    "#### Layer 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "# with tf.name_scope('hidden_layer2'):\n",
    "#     initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.01, seed=42)\n",
    "\n",
    "#     w2 = tf.Variable(initial_value=initial_w2)\n",
    "#     initial_b2 = tf.constant(value = 0.0, shape=[n_hidden2])\n",
    "#     b2 = tf.Variable(initial_value = initial_b2)\n",
    "\n",
    "#     layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "\n",
    "#     layer2_out = tf.nn.relu(layer2_in)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 90,
   "outputs": []
  },
  {
   "source": [
    "#### Output Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# with tf.name_scope('output_layer'):\n",
    "\n",
    "#     initial_w3 = tf.truncated_normal(shape=[ n_hidden2, NR_CLASSES], stddev=0.01, seed=42)\n",
    "#     w3 = tf.Variable(initial_value=initial_w3)\n",
    "\n",
    "#     initial_b3 = tf.constant(value = 0.0, shape=[NR_CLASSES])\n",
    "#     b3 = tf.Variable(initial_value = initial_b3)\n",
    "\n",
    "#     layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "\n",
    "#     output = tf.nn.softmax(layer3_in)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value=initial_w, name='W')\n",
    "\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value=initial_b, name='B')\n",
    "\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "        \n",
    "        if name=='out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "        \n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "\n",
    "        \n",
    "        \n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_drop = tf.nn.dropout(layer_1, keep_prob = 0.8, name = 'dropout_layer')\n",
    "\n",
    "                    \n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n"
   ]
  },
  {
   "source": [
    "# Tensorboard"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cannot create a file when that file already exists\n"
     ]
    }
   ],
   "source": [
    "# Folder for Tensorboard\n",
    "\n",
    "folder_name = f'Model {strftime}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Successfully created directories!')"
   ]
  },
  {
   "source": [
    "## Loss, Optimization & Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Loss Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = Y, logits=output))"
   ]
  },
  {
   "source": [
    "#### Optimizer: Adam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step  = optimizer.minimize(loss)"
   ]
  },
  {
   "source": [
    "#### Accuracy Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    model_prediction = tf.argmax(output, axis = 1, name = 'prediction')\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis = 1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)\n"
   ]
  },
  {
   "source": [
    "### Check Input Images in Tensorboard"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    tf.summary.image('image_input', x_image, max_outputs = 4)"
   ]
  },
  {
   "source": [
    "## Run Session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + '/validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "index_in_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    global num_examples\n",
    "    global index_in_epochs\n",
    "\n",
    "    start = index_in_epochs\n",
    "    index_in_epochs+=batch_size\n",
    "\n",
    "    if index_in_epochs > num_examples:\n",
    "        start = 0\n",
    "        index_in_epochs = batch_size\n",
    "\n",
    "    end = index_in_epochs\n",
    "    return data[start:end], labels[start:end]\n"
   ]
  },
  {
   "source": [
    "### Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0\t| Training Accuracy = 0.8600000143051147\n",
      "Epoch 1\t| Training Accuracy = 0.8809999823570251\n",
      "Epoch 2\t| Training Accuracy = 0.8870000243186951\n",
      "Epoch 3\t| Training Accuracy = 0.9620000123977661\n",
      "Epoch 4\t| Training Accuracy = 0.9679999947547913\n",
      "Epoch 5\t| Training Accuracy = 0.9750000238418579\n",
      "Epoch 6\t| Training Accuracy = 0.9779999852180481\n",
      "Epoch 7\t| Training Accuracy = 0.9789999723434448\n",
      "Epoch 8\t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 9\t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 10\t| Training Accuracy = 0.9819999933242798\n",
      "Epoch 11\t| Training Accuracy = 0.9860000014305115\n",
      "Epoch 12\t| Training Accuracy = 0.984000027179718\n",
      "Epoch 13\t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 14\t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 15\t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 16\t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 17\t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 18\t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 19\t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 20\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 21\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 22\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 23\t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 24\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 25\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 26\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 27\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 28\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 29\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 30\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 31\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 32\t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 33\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 34\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 35\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 36\t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 37\t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 38\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 39\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 40\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 41\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 42\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 43\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 44\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 45\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 46\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 47\t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 48\t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 49\t| Training Accuracy = 0.9919999837875366\n",
      "Training is Over!\n"
     ]
    }
   ],
   "source": [
    "#=============Training Dataset====================\n",
    "for epoch in range(nr_epochs):\n",
    "    for i in range(nr_iterations):\n",
    "        batch_x, batch_y = next_batch(batch_size = size_of_batch, data = x_train, labels = y_train)\n",
    "\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "\n",
    "        s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict = feed_dictionary)\n",
    "\n",
    "\n",
    "    train_writer.add_summary(s, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch}\\t| Training Accuracy = {batch_accuracy}')\n",
    "\n",
    "#=======================Validation===============\n",
    "\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "print(\"Training is Over!\")"
   ]
  },
  {
   "source": [
    "### Saving the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {'accuracy/prediction':model_prediction}\n",
    "inputs = {'X':X}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-108-066e40d03224>:1: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From C:\\Users\\Arhit\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: SavedModel\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.saved_model.simple_save(sess, 'SavedModel', inputs, outputs)"
   ]
  },
  {
   "source": [
    "### Prediction Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28 at 0x21709988E48>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "img = Image.open('original/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.invert(img.convert('L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sess.run(fetches = tf.argmax(output, axis=1), feed_dict = {X: [test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "source": [
    "### Reset for next run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3613jvsc74a57bd0a7ca192ce09b48752070b3ac6bafe6cc27836a639c077866addaeb19003def06",
   "display_name": "Python 3.6.13 64-bit ('TensorFlow': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "68388ac46d513204cd8f855d33290d51794fb1a60da2c6fe6ee1dabfa9739d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}